{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DATA643: Recommender System </center>\n",
    "## <center> Final Project </center>\n",
    "### <i> <center> Harpreet Shoker, Rose Koh, Summer 2018 </center> </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook4_ALS\n",
    "\n",
    "In this notebook, we perform matrix factorization using Alternating Least Squares on implicit feedback data.\n",
    "\n",
    "#### pythorn 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "datasets_path = os.path.join(os.getcwd(), 'data')\n",
    "dt_path = os.path.join(datasets_path, 'instacart_2017_05_01.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisles.csv\n",
      "departments.csv\n",
      "order_products__prior.csv\n",
      "order_products__train.csv\n",
      "orders.csv\n",
      "products.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./data/instacart_2017_05_01\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Order and User dataset\n",
    "order_products_prior = pd.read_csv('./data/instacart_2017_05_01/order_products__prior.csv')\n",
    "order_products_train = pd.read_csv('./data/instacart_2017_05_01/order_products__train.csv')\n",
    "orders = pd.read_csv('./data/instacart_2017_05_01/orders.csv')\n",
    "# Products dataset\n",
    "products = pd.read_csv('./data/instacart_2017_05_01/products.csv')\n",
    "# Merge prior orders and products\n",
    "merged_order_products_prior = pd.merge(order_products_prior, products, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(path, orders, order_products_train):\n",
    "    \"\"\"\n",
    "    Make test data and save it in the given path as .csv\n",
    "    \"\"\"\n",
    "    # read `orders` and filter eval_set == train\n",
    "    orders_train = orders.loc[(orders.eval_set == \"train\")].reset_index()\n",
    "    orders_userid = orders_train[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # `orders_userid` and `order_products_train` lengths should match\n",
    "    assert len(orders_userid[\"order_id\"].unique()) == len(order_products_train[\"order_id\"].unique())\n",
    "\n",
    "    # Convert `order_products`_train as same format\n",
    "    orders_productid = order_products_train[[\"order_id\", \"product_id\"]]\n",
    "    orders_productid = orders_productid.groupby(\"order_id\")[\"product_id\"].apply(list).reset_index().rename(columns={\"product_id\": \"products\"})\n",
    "\n",
    "    # `orders_products_train` and `orders_productid` size should match\n",
    "    assert orders_productid.size == orders_userid.size\n",
    "\n",
    "    # merge `orders_userid` and `orders_productid` on order_id\n",
    "    user_products_test = pd.merge(orders_userid, orders_productid, on=\"order_id\")\n",
    "    user_products_test = user_products_test[[\"user_id\", \"products\"]]\n",
    "\n",
    "    # save as .csv\n",
    "    user_products_test.to_csv(path, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 35.3 ms, total: 219 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate test data if it doesn't exist\n",
    "if_test_data_exists = False\n",
    "test_data_path = \"./data/user_products__test.csv\"\n",
    "\n",
    "if if_test_data_exists or not Path(test_data_path).is_file():\n",
    "    test_data(test_data_path, orders, order_products_train)\n",
    "\n",
    "user_products_test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131209, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[196, 25133, 38928, 26405, 39657, 10258, 13032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[22963, 7963, 16589, 32792, 41787, 22825, 1364...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[15349, 19057, 16185, 21413, 20843, 20114, 482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>[12053, 47272, 37999, 13198, 43967, 40852, 176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[15937, 5539, 10960, 23165, 22247, 4853, 27104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           products\n",
       "0        1  [196, 25133, 38928, 26405, 39657, 10258, 13032...\n",
       "1        2  [22963, 7963, 16589, 32792, 41787, 22825, 1364...\n",
       "2        5  [15349, 19057, 16185, 21413, 20843, 20114, 482...\n",
       "3        7  [12053, 47272, 37999, 13198, 43967, 40852, 176...\n",
       "4        8  [15937, 5539, 10960, 23165, 22247, 4853, 27104..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(user_products_test_df.shape)\n",
    "user_products_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_prior_df(path, orders, order_products_prior):\n",
    "    \"\"\"\n",
    "    Make prior user-product dataframe and save it as .csv\n",
    "    \"\"\"   \n",
    "    \n",
    "    # read `orders` and filter eval_set == prior\n",
    "    orders_user_prior = orders.loc[orders.eval_set == \"prior\"]\n",
    "    orders_user_prior = orders_user_prior[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # merge `orders_user_prior` and `order_products_prior` on order_id\n",
    "    merged = pd.merge(orders_user_prior, order_products_prior[[\"order_id\", \"product_id\"]], on=\"order_id\")\n",
    "    user_item_prior = merged[[\"user_id\", \"product_id\"]]\n",
    "    user_item_prior = user_item_prior.groupby([\"user_id\", \"product_id\"]).size().reset_index().rename(columns={0:\"quantity\"})\n",
    "    \n",
    "    # save as .csv\n",
    "    user_item_prior.to_csv(path, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 10.1 s, total: 1min 28s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate users prior purchases data if it doesn't exist\n",
    "if_user_prod_df_exists = True\n",
    "matrix_df_path = \"./data/user_products__prior.csv\"\n",
    "\n",
    "if if_user_prod_df_exists or not Path(matrix_df_path).is_file():\n",
    "    user_item_prior_df(matrix_df_path, orders, order_products_prior)\n",
    "\n",
    "user_item_prior = pd.read_csv(matrix_df_path)\n",
    "user_item_prior[\"user_id\"] = user_item_prior[\"user_id\"].astype(\"category\")\n",
    "user_item_prior[\"product_id\"] = user_item_prior[\"product_id\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13307953, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10258</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13032</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id product_id  quantity\n",
       "0       1        196        10\n",
       "1       1      10258         9\n",
       "2       1      10326         1\n",
       "3       1      12427        10\n",
       "4       1      13032         3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(user_item_prior.shape)\n",
    "user_item_prior.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "\n",
    "def build_user_item_matrix(path, user_item_prior):\n",
    "    \"\"\"\n",
    "    make user-item matrix that displays order history of users, save it as .csv\n",
    "    rows = products\n",
    "    columns = users\n",
    "    \"\"\"\n",
    "    user_item_matrix = sparse.coo_matrix((user_item_prior[\"quantity\"],\n",
    "                                          (user_item_prior[\"product_id\"].cat.codes.copy(),\n",
    "                                           user_item_prior[\"user_id\"].cat.codes.copy())))    \n",
    "    sparse.save_npz(path, user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of users, products and quantity bought using prior datasets\n",
    "if_user_item_matrix_exists = False\n",
    "matrix_path = \"./data/user_item_matrix.npz\"\n",
    "\n",
    "if if_user_item_matrix_exists or not Path(matrix_path).is_file():\n",
    "    build_user_item_matrix(matrix_path, user_item_prior)  \n",
    "\n",
    "user_item_matrix=sparse.load_npz(matrix_path).tocsr().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49677x206209 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 13307953 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix.shape\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity of user_item_matrix is 99.8701%\n"
     ]
    }
   ],
   "source": [
    "sparsity = (1 - (user_item_matrix.size / (user_item_matrix.shape[0] * user_item_matrix.shape[1])))\n",
    "print(('The sparsity of user_item_matrix is ') +  str(round(sparsity,6)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Alternate Least Squares - Implicit Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "def confidence_matrix(user_item_matrix, alpha):\n",
    "    \"\"\"\n",
    "    Given user-item-matrix, returns the given matrix converted to a confidence matrix.\n",
    "    \"\"\"\n",
    "    return (user_item_matrix * alpha).astype(\"float\")\n",
    "\n",
    "\n",
    "def build_implicit_matrix_factorization(user_item_matrix, **kwargs):\n",
    "    \"\"\"\n",
    "    Given user-item-matrix and model parameters, builds models and save.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build model\n",
    "    model = AlternatingLeastSquares()\n",
    "    model.approximate_similar_items = False\n",
    "    \n",
    "    model.fit(confidence_matrix(user_item_matrix, kwargs[\"alpha\"]))\n",
    "\n",
    "    # Save model\n",
    "    with open(kwargs[\"path\"], \"wb+\") as f:\n",
    "        pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model params and build it\n",
    "model_params = {\"alpha\": 23} \n",
    "model_params[\"path\"] = \"./models/implicit_matrix_factorization/{}.imf\".format(model_params[\"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_model_exists = False\n",
    "if if_model_exists or not Path(model_params[\"path\"]).exists():\n",
    "    build_implicit_matrix_factorization(user_item_matrix, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_params[\"path\"], \"rb\") as f:\n",
    "    imf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the user-item matrix is 0 indexed, the dict is required to convert between `ids` and `indices`\n",
    "# e.g. `product_id` 1 represents by the `0`th row of the user-item matrix.\n",
    "\n",
    "# Maps user_id: user index\n",
    "u_dict = {uid:i for i, uid in enumerate(user_item_prior[\"user_id\"].cat.categories)}\n",
    "\n",
    "# Maps product_index: product id\n",
    "p_dict = dict(enumerate(user_item_prior[\"product_id\"].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend items for a user 23\n",
    "user_id = 23\n",
    "recommendations = imf_model.recommend(u_dict[user_id], user_item_matrix.T.tocsr(), N = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual purchase list of User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER 23 PURCHASE LIST: \n",
      "\n",
      "['3 Color Deli Coleslaw', 'Pineapple on the Bottom Greek Yogurt', 'Original French Vanilla Yogurt', 'Febreze Lavender Vanilla & Comfort Scent Sweeper Dry Pad Refills', 'Recipe Secrets Onion Soup & Dip Mix', 'Extra Noodle Soup Mix', 'Chicken Bouillon Cubes', 'Natural Goodness 33% Less Sodium Chicken Broth', 'Low Fat Key Lime Blended Greek Yogurt', 'Original Mountain Blueberry Low Fat Yogurt', '3 Gallon Food Scrap Bag', 'Spinach']\n"
     ]
    }
   ],
   "source": [
    "# Actual Purchase List\n",
    "row = user_products_test_df.loc[user_products_test_df.user_id == user_id]\n",
    "actual = list(row[\"products\"])\n",
    "actual = actual[0][1:-1]\n",
    "actual = list(np.array([p.strip() for p in actual.strip().split(\",\")]).astype(np.int64))\n",
    "\n",
    "actual_products = []\n",
    "for pid in actual:\n",
    "    actual_products.extend((products.loc[products.product_id == pid].product_name).tolist())\n",
    "\n",
    "print(\"\\nUSER {} PURCHASE LIST: \\n\\n{}\".format(user_id, actual_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommended purchase list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for USER 23\n",
      "['Peach on the Bottom Nonfat Greek Yogurt', 'Strawberry on the Bottom Nonfat Greek Yogurt', 'Blueberry on the Bottom Nonfat Greek Yogurt', 'Raspberry on the Bottom Nonfat Greek Yogurt', 'Non Fat Black Cherry on the Bottom Greek Yogurt', '100% Whole Wheat Bread', 'Low-Fat Strawberry Banana on the Bottom Greek Yogurt', 'Hass Avocado', 'Original No Pulp 100% Florida Orange Juice', 'Coconut Blended Greek Yogurt']\n"
     ]
    }
   ],
   "source": [
    "# Recommended List\n",
    "r = [p_dict[r[0]] for r in recommendations] # Takes the product_cat_code and maps to product_id\n",
    "\n",
    "recommended_products = []\n",
    "for pid in r:\n",
    "    recommended_products.extend((products.loc[products.product_id == pid].product_name).tolist())\n",
    "print(\"\\nRecommendations for USER {}\\n{}\".format(user_id, recommended_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "<h5> We can state that similarities shows in the purchase list of the `User 23` and the recommended list.  For example, the recommended Nonfat yogurts are appropriate alternative to purchased yogurt.  This recommender system is discovery based, thus, it recommends products that have never been purchased. As a result, for evaluation, we remove previously purchased products from the actual purchase list of the user. </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using `Recall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_popular(k, merged_order_products_prior):\n",
    "    \"\"\"\n",
    "    Returns the `k` most popular products based on purchase count in the dataset\n",
    "    \"\"\"\n",
    "    popular_products = list(merged_order_products_prior[\"product_id\"].value_counts().head(k).index)\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the product_user utility matrix\n",
    "user_product_matrix = user_item_matrix.T.tocsr()\n",
    "\n",
    "# Number of recommendations\n",
    "number_of_recommendations = 10\n",
    "\n",
    "# Get the `number_of_recommendations` most popular products\n",
    "popular_products = get_k_popular(number_of_recommendations, merged_order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(actual, pred):\n",
    "    \"\"\"\n",
    "    Given the actual, prediction values of list, returns the recall of the prediction.\n",
    "    \"\"\"\n",
    "    if len(actual) == 0:\n",
    "        return 0\n",
    "    actual, pred = set(actual), set(pred)\n",
    "    return len(actual.intersection(pred)) / len(actual)\n",
    "\n",
    "\n",
    "def new_products(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset, returns the list of newly purchased products.\n",
    "    \"\"\"\n",
    "    actual = row[\"products\"][1:-1]                                                          # Products purchased currently \n",
    "    actual = set([int(p.strip()) for p in actual.strip().split(\",\")])\n",
    "    liked = set([p_dict[i] for i in user_product_matrix[u_dict[row[\"user_id\"]]].indices])   # User's purchase history\n",
    "    return actual - liked                                                                   # Return only new products purchased\n",
    "\n",
    "\n",
    "def popular_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset, returns the recall score when popular products are recommended.\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    return recall_score(actual, popular_products)\n",
    "\n",
    "\n",
    "def imf_recommend(row):\n",
    "    \"\"\"\n",
    "    Given a row in the test dataset, returns the recall score when our model recommends products.\n",
    "    \"\"\"\n",
    "    actual = new_products(row)\n",
    "    recommended = imf_model.recommend(u_dict[row[\"user_id\"]], user_product_matrix, N=number_of_recommendations)\n",
    "    recommended = [p_dict[r[0]] for r in recommended]\n",
    "    return recall_score(actual, recommended)\n",
    "\n",
    "\n",
    "def build_eval_df(user_products_test_df, filepath=None, subset=None):\n",
    "    \"\"\"\n",
    "    Builds recall values dataframe of the baseline and implicit matrix factorization model\n",
    "    for all the users in the test dataset and save it as .csv.\n",
    "    \"\"\"\n",
    "\n",
    "    df_eval = user_products_test_df.copy()\n",
    "    if subset:\n",
    "        df_eval = df_eval.sample(n=int(len(df_eval) * subset), random_state=7)\n",
    "    df_eval[\"popular_score\"] = df_eval.apply(popular_recommend, axis=1)\n",
    "    df_eval[\"imf_score\"] = df_eval.apply(imf_recommend, axis=1)\n",
    "\n",
    "    df_eval.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall value dataframe of the baseline and implicit matrix factorization model\n",
    "\n",
    "if_evaluation_df_exists = True\n",
    "subset = 0.2  # Evaluate on `subset x 100`% of the test dataset\n",
    "eval_path = \"./data/eval/eval_discovery_{}_{}.csv\".format(subset if subset is not None else \"full\", number_of_recommendations)\n",
    "\n",
    "if if_evaluation_df_exists or not Path(eval_path).exists():\n",
    "    build_eval_df(user_products_test_df, filepath=eval_path, subset=subset)\n",
    "df_eval = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit Matrix Factorization Model: 3.83%\n",
      "Baseline: 2.62%\n"
     ]
    }
   ],
   "source": [
    "# Mean recall scores\n",
    "model_mean_recall, baseline_mean_recall = np.mean(df_eval[\"imf_score\"]), np.mean(df_eval[\"popular_score\"])\n",
    "print(\"Implicit Matrix Factorization Model: {:.2f}%\".format(model_mean_recall * 100))\n",
    "print(\"Baseline: {:.2f}%\".format(baseline_mean_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implicit matrix factorization model performss better than baseline model for recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS with Spark ML library \n",
    "\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares. The implementation in MLlib has these parameters:\n",
    "\n",
    "* numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "* rank is the number of latent factors in the model.\n",
    "* iterations is the number of iterations to run.\n",
    "* lambda specifies the regularization parameter in ALS.\n",
    "* implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\n",
    "* alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\n",
    "\n",
    "See documentation at https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS with Spark ML library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Start Spark session with local master and 2 cores\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"ALS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_test_data(path, orders, order_products_train):\n",
    "    \"\"\"\n",
    "    Make spark test data for spark and save it in the given path as .csv\n",
    "    \"\"\"\n",
    "    # read `orders` and filter eval_set == train\n",
    "    orders_train = orders.loc[(orders.eval_set == \"train\")].reset_index()\n",
    "    orders_userid = orders_train[[\"order_id\", \"user_id\"]]\n",
    "    \n",
    "    # Convert `order_products`_train as same format\n",
    "    orders_productid = order_products_train[[\"order_id\", \"product_id\"]]\n",
    "\n",
    "    # merge `orders_userid` and `orders_productid` on order_id\n",
    "    user_products_test = pd.merge(orders_userid, orders_productid, on=\"order_id\")\n",
    "    user_products_test = user_products_test[[\"user_id\", \"product_id\"]]\n",
    "\n",
    "    # save as .csv\n",
    "    user_products_test.to_csv(path, index_label=False)\n",
    "\n",
    "# Generate spark test data if it doesn't exist\n",
    "if_spark_test_data_exists = False\n",
    "spark_test_data_path = \"./data/spark_user_products__test.csv\"\n",
    "\n",
    "if if_spark_test_data_exists or not Path(spark_test_data_path).is_file():\n",
    "    spark_test_data(spark_test_data_path, orders, order_products_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 ms, sys: 2.14 ms, total: 5.79 ms\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in user_item_prior from \"./data/user_products__prior.csv\"\n",
    "train = spark.read.csv(matrix_df_path, inferSchema=True, header=True)\n",
    "# read in test data from \"./data/spark_user_products__test.csv\"\n",
    "test = spark.read.csv(spark_test_data_path, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 ms, sys: 16 ms, total: 43.9 ms\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.1, implicitPrefs=True, nonnegative=True,\\\n",
    "          coldStartStrategy=\"drop\",\\\n",
    "          userCol='user_id', itemCol='product_id', ratingCol='quantity')\n",
    "\n",
    "als.setSeed(23)\n",
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|user_id|product_id|prediction|\n",
      "+-------+----------+----------+\n",
      "|   1046|       148|       0.0|\n",
      "|   1047|       148|       0.0|\n",
      "|   1048|       148|       0.0|\n",
      "|   1045|       148|       0.0|\n",
      "|   1049|       148|       0.0|\n",
      "|   1043|       148|       0.0|\n",
      "|   1044|       148|       0.0|\n",
      "|   3089|       471|       0.0|\n",
      "|   3087|       471|       0.0|\n",
      "|   3076|       471|       0.0|\n",
      "|   3083|       471|       0.0|\n",
      "|   3095|       471|       0.0|\n",
      "|   3084|       471|       0.0|\n",
      "|   3097|       471|       0.0|\n",
      "|   3080|       471|       0.0|\n",
      "|   3086|       471|       0.0|\n",
      "|   3092|       471|       0.0|\n",
      "|   3085|       471|       0.0|\n",
      "|   3093|       471|       0.0|\n",
      "|   3090|       471|       0.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.transform(test)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
